{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMR Stac ➡ DuckDB\n",
    "Based on `stac_geoparquet.ipynb` and `duck_machine.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import stac_geoparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = 'LPCLOUD'\n",
    "short_name = 'MCD19A2_061'\n",
    "page_size = 100\n",
    "max_records = 2000\n",
    "\n",
    "base_path = f\"data/{provider}/{short_name}\"\n",
    "os.makedirs(base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pages of STAC Items\n",
    "\n",
    "For each FeatureCollection response from the STAC Items endpoint, write each of its features into a `jsonl` file. Append to this file for each FeatureCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing lines for 0-100\n",
      "Finished writing lines for 100-200\n",
      "Finished writing lines for 200-300\n",
      "Finished writing lines for 300-400\n",
      "Finished writing lines for 400-500\n",
      "Finished writing lines for 500-600\n",
      "Finished writing lines for 600-700\n",
      "Finished writing lines for 700-800\n",
      "Finished writing lines for 800-900\n",
      "Finished writing lines for 900-1000\n",
      "Finished writing lines for 1000-1100\n",
      "Finished writing lines for 1100-1200\n",
      "Finished writing lines for 1200-1300\n",
      "Finished writing lines for 1300-1400\n",
      "Finished writing lines for 1400-1500\n",
      "Finished writing lines for 1500-1600\n",
      "Finished writing lines for 1600-1700\n",
      "Finished writing lines for 1700-1800\n",
      "Finished writing lines for 1800-1900\n",
      "Reached max_records = 2000\n"
     ]
    }
   ],
   "source": [
    "base = f\"https://cmr.earthdata.nasa.gov/stac/{provider}/collections/{short_name}/items?limit={page_size}\"\n",
    "current = base\n",
    "batch_count = 0\n",
    "\n",
    "stac_out_basename = f\"{short_name}_{max_records}\"\n",
    "stac_out_path = os.path.join(base_path, stac_out_basename + \".jsonl\")\n",
    "\n",
    "with open (stac_out_path, 'w') as jsonl:\n",
    "  while len(current) > 0:\n",
    "      batch_count += 1\n",
    "      resp = requests.get(current)\n",
    "\n",
    "      # check for end of data\n",
    "      if resp.status_code!=200:\n",
    "          message = resp.json()['errors'][0]\n",
    "          print(f\"\\033(31mError: {resp.status_code} - {resp.reason}: {message}\\n{current}\\033(0m\")\n",
    "          print(f\"On batch number {batch_count}\")\n",
    "          break # the cursor is broken, get out\n",
    "\n",
    "\n",
    "      # write items from collection batch as json lines\n",
    "      for item in data['features']:\n",
    "        json.dump(item, jsonl, separators=(\",\", \":\"))\n",
    "        jsonl.write(\"\\n\")\n",
    "\n",
    "      # exit if reached maximum record count\n",
    "      if page_size * batch_count >= max_records:\n",
    "        print(f\"Reached max_records = {max_records}\")\n",
    "        break\n",
    "      else:\n",
    "        print(f\"Finished writing lines for {(batch_count - 1) * page_size}-{batch_count * page_size}\")\n",
    "\n",
    "      # look for next page\n",
    "      data = resp.json()\n",
    "      for link in data['links']:\n",
    "          if link['rel']=='next':\n",
    "              next = link['href']\n",
    "              current = next if current != next else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIS Sinusoidal Grid\n",
    "\n",
    "![](https://lpdaac.usgs.gov/media/images/modis_sinusoidal_grid.width-800.jpg)\n",
    "\n",
    "How many unique MODIS SIN grid tiles (unique geometries) are in this query?\n",
    "\n",
    "The [maximum unique tiles would be 460](https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/modis-overview/#modis-tiling-systems), but not every collection would include data for every tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     163\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$stac_out_path\"\n",
    "grep -o -e 'h[01].v[01].' $1 | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSONL output as Arrow table\n",
    "\n",
    "Then check schema and dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrow table shape:    (2000, 13)\n",
      "Arrow table columns: \n",
      "   ['assets',\n",
      "   'bbox',\n",
      "   'collection',\n",
      "   'geometry',\n",
      "   'id',\n",
      "   'links',\n",
      "   'stac_extensions',\n",
      "   'stac_version',\n",
      "   'type',\n",
      "   'datetime',\n",
      "   'end_datetime',\n",
      "   'eo:cloud_cover',\n",
      "   'start_datetime']\n"
     ]
    }
   ],
   "source": [
    "record_batch_reader = stac_geoparquet.arrow.parse_stac_ndjson_to_arrow(stac_out_path)\n",
    "table = record_batch_reader.read_all()\n",
    "print(\"Arrow table shape:   \", table.shape)\n",
    "print(\"Arrow table columns: \\n  \", str(table.column_names).replace(',', ',\\n  '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from JSONL to Geoparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_parquet_path = os.path.join(base_path, stac_out_basename + \".parquet\")\n",
    "stac_geoparquet.arrow.parse_stac_ndjson_to_parquet(stac_out_path, out_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm Geoparquet output has same structure as Arrow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet table shape:    (2000, 13)\n",
      "Parquet table columns: \n",
      "   ['assets',\n",
      "   'bbox',\n",
      "   'collection',\n",
      "   'geometry',\n",
      "   'id',\n",
      "   'links',\n",
      "   'stac_extensions',\n",
      "   'stac_version',\n",
      "   'type',\n",
      "   'datetime',\n",
      "   'end_datetime',\n",
      "   'eo:cloud_cover',\n",
      "   'start_datetime']\n"
     ]
    }
   ],
   "source": [
    "pq_table = pq.read_table(out_parquet_path)\n",
    "print(\"Parquet table shape:   \", pq_table.shape)\n",
    "print(\"Parquet table columns: \\n  \", str(pq_table.column_names).replace(',', ',\\n  '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify compliance with `gpq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary: Passed 20 checks.\n",
      "\n",
      " ✓ file must include a \"geo\" metadata key\n",
      " ✓ metadata must be a JSON object\n",
      " ✓ metadata must include a \"version\" string\n",
      " ✓ metadata must include a \"primary_column\" string\n",
      " ✓ metadata must include a \"columns\" object\n",
      " ✓ column metadata must include the \"primary_column\" name\n",
      " ✓ column metadata must include a valid \"encoding\" string\n",
      " ✓ column metadata must include a \"geometry_types\" list\n",
      " ✓ optional \"crs\" must be null or a PROJJSON object\n",
      " ✓ optional \"orientation\" must be a valid string\n",
      " ✓ optional \"edges\" must be a valid string\n",
      " ✓ optional \"bbox\" must be an array of 4 or 6 numbers\n",
      " ✓ optional \"epoch\" must be a number\n",
      " ✓ geometry columns must not be grouped\n",
      " ✓ geometry columns must be stored using the BYTE_ARRAY parquet type\n",
      " ✓ geometry columns must be required or optional, not repeated\n",
      " ✓ all geometry values match the \"encoding\" metadata\n",
      " ✓ all geometry types must be included in the \"geometry_types\" metadata (if not empty)\n",
      " ✓ all polygon geometries must follow the \"orientation\" metadata (if present)\n",
      " ✓ all geometries must fall within the \"bbox\" metadata (if present)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$out_parquet_path\"\n",
    "gpq validate $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Geoparquet file with DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test point in Canada. Should only match `h11v03`, but matches at least 3 other tiles, too.  \n",
    "This point is very far (>260km) from any borders of the MODIS grid tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬───────────────────────────────────────────┬──────────────────────────┐\n",
       "│  found  │                    id                     │         datetime         │\n",
       "│ boolean │                  varchar                  │ timestamp with time zone │\n",
       "├─────────┼───────────────────────────────────────────┼──────────────────────────┤\n",
       "│ true    │ MCD19A2.A2000058.h08v03.061.2022153225217 │ 2000-02-26 19:40:00-05   │\n",
       "│ true    │ MCD19A2.A2000058.h09v03.061.2022153225142 │ 2000-02-26 19:40:00-05   │\n",
       "│ true    │ MCD19A2.A2000058.h07v03.061.2022153223310 │ 2000-02-26 19:45:00-05   │\n",
       "│ true    │ MCD19A2.A2000055.h07v03.061.2022153215825 │ 2000-02-23 19:10:00-05   │\n",
       "│ true    │ MCD19A2.A2000055.h09v03.061.2022153221548 │ 2000-02-23 19:10:00-05   │\n",
       "│ true    │ MCD19A2.A2000055.h08v03.061.2022153222217 │ 2000-02-23 19:10:00-05   │\n",
       "│ true    │ MCD19A2.A2000055.h11v03.061.2022153220110 │ 2000-02-24 11:45:00-05   │\n",
       "│ true    │ MCD19A2.A2000056.h09v03.061.2022153222246 │ 2000-02-24 19:55:00-05   │\n",
       "│ true    │ MCD19A2.A2000056.h08v03.061.2022153223911 │ 2000-02-24 19:55:00-05   │\n",
       "│ true    │ MCD19A2.A2000056.h11v03.061.2022153222119 │ 2000-02-25 14:05:00-05   │\n",
       "│  ·      │                     ·                     │           ·              │\n",
       "│  ·      │                     ·                     │           ·              │\n",
       "│  ·      │                     ·                     │           ·              │\n",
       "│ true    │ MCD19A2.A2000059.h11v03.061.2022153230554 │ 2000-02-28 12:55:00-05   │\n",
       "│ true    │ MCD19A2.A2000059.h07v03.061.2022153223821 │ 2000-02-28 16:15:00-05   │\n",
       "│ true    │ MCD19A2.A2000059.h08v03.061.2022153225731 │ 2000-02-28 16:15:00-05   │\n",
       "│ true    │ MCD19A2.A2000059.h09v03.061.2022153225639 │ 2000-02-28 16:15:00-05   │\n",
       "│ true    │ MCD19A2.A2000060.h07v03.061.2022153224934 │ 2000-02-28 19:30:00-05   │\n",
       "│ true    │ MCD19A2.A2000060.h08v03.061.2022153230636 │ 2000-02-28 19:30:00-05   │\n",
       "│ true    │ MCD19A2.A2000060.h09v03.061.2022153230716 │ 2000-02-28 19:30:00-05   │\n",
       "│ true    │ MCD19A2.A2000060.h11v03.061.2022153232503 │ 2000-02-29 12:00:00-05   │\n",
       "│ true    │ MCD19A2.A2000061.h08v03.061.2022153231502 │ 2000-02-29 20:10:00-05   │\n",
       "│ true    │ MCD19A2.A2000061.h09v03.061.2022153233509 │ 2000-02-29 20:10:00-05   │\n",
       "├─────────┴───────────────────────────────────────────┴──────────────────────────┤\n",
       "│ 29 rows (20 shown)                                                   3 columns │\n",
       "└────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(f'''\n",
    "SELECT st_contains(geometry::geometry, 'POINT(-107.5 53.3)'::GEOMETRY) as found,\n",
    "    id,\n",
    "    datetime\n",
    "FROM parquet_scan('{out_parquet_path}')\n",
    "WHERE found == true\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSU Test Point\n",
    "Test point on the Oval of Ohio State University, Columbus. Should only match `h11v04`, but also matches `h11v05` to the south.  \n",
    "This point is only 4km north of the border, but I would have expected east-west boundaries to have more potential for inaccurate intersections than north-south ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬───────────────────────────────────────────┬──────────────────────────┐\n",
       "│  found  │                    id                     │         datetime         │\n",
       "│ boolean │                  varchar                  │ timestamp with time zone │\n",
       "├─────────┼───────────────────────────────────────────┼──────────────────────────┤\n",
       "│ true    │ MCD19A2.A2000055.h11v05.061.2022153215223 │ 2000-02-24 10:10:00-05   │\n",
       "│ true    │ MCD19A2.A2000055.h11v04.061.2022153215336 │ 2000-02-24 11:45:00-05   │\n",
       "│ true    │ MCD19A2.A2000056.h11v05.061.2022153221329 │ 2000-02-25 10:50:00-05   │\n",
       "│ true    │ MCD19A2.A2000056.h11v04.061.2022153221449 │ 2000-02-25 10:50:00-05   │\n",
       "│ true    │ MCD19A2.A2000057.h11v05.061.2022153222247 │ 2000-02-26 10:00:00-05   │\n",
       "│ true    │ MCD19A2.A2000057.h11v04.061.2022153223331 │ 2000-02-26 11:30:00-05   │\n",
       "│ true    │ MCD19A2.A2000058.h11v05.061.2022153223828 │ 2000-02-27 10:40:00-05   │\n",
       "│ true    │ MCD19A2.A2000058.h11v04.061.2022153225015 │ 2000-02-27 10:40:00-05   │\n",
       "│ true    │ MCD19A2.A2000059.h11v05.061.2022153224707 │ 2000-02-28 11:20:00-05   │\n",
       "│ true    │ MCD19A2.A2000059.h11v04.061.2022153230137 │ 2000-02-28 11:20:00-05   │\n",
       "│ true    │ MCD19A2.A2000060.h11v05.061.2022153225620 │ 2000-02-29 10:25:00-05   │\n",
       "│ true    │ MCD19A2.A2000060.h11v04.061.2022153231515 │ 2000-02-29 12:05:00-05   │\n",
       "├─────────┴───────────────────────────────────────────┴──────────────────────────┤\n",
       "│ 12 rows                                                              3 columns │\n",
       "└────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "duckdb.sql(f'''\n",
    "SELECT st_contains(geometry::geometry, 'POINT(-83.0123 40)'::GEOMETRY) as found,\n",
    "    id,\n",
    "    datetime\n",
    "FROM parquet_scan('{out_parquet_path}')\n",
    "WHERE found == true\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSU Test Point\n",
    "Test point at Penn State University's Berkey Creamery. Should only match `h12v04`, and does.  \n",
    "While this point is relatively near to the southwest corner of the MODIS grid tile, it is still nearly 100km away from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬───────────────────────────────────────────┬──────────────────────────┐\n",
       "│  found  │                    id                     │         datetime         │\n",
       "│ boolean │                  varchar                  │ timestamp with time zone │\n",
       "├─────────┼───────────────────────────────────────────┼──────────────────────────┤\n",
       "│ true    │ MCD19A2.A2000055.h12v04.061.2022153215604 │ 2000-02-24 10:05:00-05   │\n",
       "│ true    │ MCD19A2.A2000056.h12v04.061.2022153221624 │ 2000-02-25 10:50:00-05   │\n",
       "│ true    │ MCD19A2.A2000057.h12v04.061.2022153222819 │ 2000-02-26 09:55:00-05   │\n",
       "│ true    │ MCD19A2.A2000058.h12v04.061.2022153224130 │ 2000-02-27 10:35:00-05   │\n",
       "│ true    │ MCD19A2.A2000059.h12v04.061.2022153225050 │ 2000-02-28 09:45:00-05   │\n",
       "│ true    │ MCD19A2.A2000060.h12v04.061.2022153225921 │ 2000-02-29 10:25:00-05   │\n",
       "└─────────┴───────────────────────────────────────────┴──────────────────────────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(f'''\n",
    "SELECT st_contains(geometry::geometry, 'POINT(-77.862 40.803)'::GEOMETRY) as found,\n",
    "    id,\n",
    "    datetime\n",
    "FROM parquet_scan('{out_parquet_path}')\n",
    "WHERE found == true\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigstac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
