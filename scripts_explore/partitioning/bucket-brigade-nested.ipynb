{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Brigade - Nested edition\n",
    "\n",
    "Bucket Brigade is a system for taking a large parquet file breaking it up spatially across multiple files using geohash to organize files into nested directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "#import dask_geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from shapely import wkb, wkt\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import Polygon\n",
    "import pygeohash as pgh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a GEOHash\n",
    "\n",
    "Lets take a second to look at geo hash and what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pgh.encode(latitude=42.6, longitude=-5.6))\n",
    "print(pgh.encode(latitude=42.6, longitude=-5.6, precision=5))\n",
    "print(pgh.encode(latitude=32.0, longitude=-7.0, precision=5))\n",
    "\n",
    "print(pgh.decode(geohash='ezs42'))\n",
    "print(pgh.geohash_approximate_distance(geohash_1='bcd3u', geohash_2='bc83n') /1000 ,\"km\")\n",
    "#not found in lib???\n",
    "#print(pgh.get_adjacent(geohash='kd3ybyu', direction='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Pick a file and then read it in using GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file = \"~/src/project/cmr-bigstac-prototype/bigstac/scripts_explore/3mil_no_global_bounds.parquet\"\n",
    "\n",
    "# Read the GeoParquet file\n",
    "gdf = gpd.read_parquet(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions\n",
    "\n",
    "going to use these things latter on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilities\n",
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        #print(f\"Successfully wrote to the file {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file {filename}: {e}\")\n",
    "\n",
    "def make_geo_box(sub:str, details:dict):\n",
    "  \"\"\"Make a directory to store parquet files in\"\"\"\n",
    "  data = f\"{os.getcwd()}/data2/{sub}\"\n",
    "  #print(data)\n",
    "  if not os.path.exists(data):\n",
    "    os.makedirs(data)\n",
    "    write_string_to_file(f\"{data}/info.json\", json.dumps(details))\n",
    "\n",
    "def hash_to_box(hash1:str, hash2:str) -> box:\n",
    "  miny, minx = pgh.decode(hash1)\n",
    "  maxy, maxx = pgh.decode(hash2)\n",
    "  return box(minx, miny, maxx, maxy)\n",
    "\n",
    "def find_hemispheres(bound_obj:Polygon) -> str:\n",
    "    minx, miny, maxx, maxy = bound_obj.bounds\n",
    "    #print(minx, miny, maxx, maxy)\n",
    "    hemispheres = set()\n",
    "\n",
    "    if minx < 0:\n",
    "        if miny < 0:\n",
    "            hemispheres.add(\"SW\")\n",
    "        if maxy > 0:\n",
    "            hemispheres.add(\"NW\")\n",
    "    if maxx > 0:\n",
    "        if miny < 0:\n",
    "            hemispheres.add(\"SE\")\n",
    "        if maxy > 0:\n",
    "            hemispheres.add(\"NE\")\n",
    "\n",
    "    if len(hemispheres) == 0:\n",
    "        # The box is exactly on the equator and prime meridian\n",
    "        return \"Central\"\n",
    "    elif len(hemispheres) == 4:\n",
    "        return \"All\"\n",
    "    else:\n",
    "        return '-'.join(hemispheres)\n",
    "\n",
    "#print(find_hemispheres(hash_to_box('9hr', 'f8b')))\n",
    "\n",
    "def is_global(minx, miny, maxx, maxy):\n",
    "  if minx < -180 or maxx > 180 or miny < -90 or maxy > 90:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def is_global_from_geohash(hash1, hash2):\n",
    "  is_global_from_geometry(hash_to_box(hash1, hash2))\n",
    "\n",
    "def is_global_from_geometry(geometry_obj):\n",
    "  minx, miny, maxx, maxy = geometry_obj.bounds\n",
    "  is_global(minx, miny, maxx, maxy)\n",
    "\n",
    "def hash_to_path(hash1, hash2):\n",
    "  hash_path = ''\n",
    "  # test if box fits in a bin\n",
    "  if hash1 == hash2:\n",
    "    # exact match, three down\n",
    "    hash_path = f\"{hash1[0]}/{hash1[1]}/{hash1[2]}\"\n",
    "  elif hash1[0] == hash2[0] and hash1[1] == hash2[1]:\n",
    "    # match 2 layers down\n",
    "    hash_path = f\"{hash1[0]}/{hash1[1]}\"\n",
    "  elif hash1[0] == hash2[0]:\n",
    "    # match 1 layer down\n",
    "    hash_path = f\"{hash1[0]}\"\n",
    "  elif is_global_from_geohash(hash1, hash2):\n",
    "    # global\n",
    "      hash_path = \"global\"\n",
    "  else:\n",
    "    # does not fit in just one box, check hemispheres\n",
    "    hash_path = find_hemispheres(hash_to_box(hash1, hash2))\n",
    "  \n",
    "  return hash_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your really worried about the code above, run this otherwise minimize it and forget it\n",
    "def test_find_hemispheres():\n",
    "  assert find_hemispheres(box(-10, -10, 1, 1)) == 'All'\n",
    "  assert find_hemispheres(box(-10, -10, -5, -5)) == 'SW'\n",
    "  assert find_hemispheres(box(5, -10, 10, -5)) == 'SE'\n",
    "  assert find_hemispheres(box(-10, 5, -5, 10)) == 'NW'\n",
    "  assert find_hemispheres(box(5, 5, 10, 10)) == 'NE'\n",
    "  assert find_hemispheres(box(-10, -10, 10, 0)) == 'SE-SW'\n",
    "  assert find_hemispheres(box(-10, 0, 10, 10)) == 'NE-NW'\n",
    "  assert find_hemispheres(box(-10, -10, 0, 10)) == 'NW-SW'\n",
    "  assert find_hemispheres(box(0, -10, 10, 10)) == 'SE-NE'\n",
    "  assert find_hemispheres(box(-10, 0, 0, 10)) == 'NW'\n",
    "  assert find_hemispheres(box(0, -10, 10, 0)) == 'SE'\n",
    "  assert find_hemispheres(box(-10, -10, 0, 0)) == 'SW'\n",
    "  assert find_hemispheres(box(0, 0, 10, 10)) == 'NE'\n",
    "  assert find_hemispheres(box(-10, 0, 0, 10)) == 'NW'\n",
    "  assert find_hemispheres(box(0, -10, 10, 0)) == 'SE'\n",
    "  assert find_hemispheres(box(-10, -10, 0, 0)) == 'SW'\n",
    "  assert find_hemispheres(box(0, 0, 10, 10)) == 'NE'\n",
    "  assert find_hemispheres(box(-10, 0, 0, 10)) == 'NW'\n",
    "  assert find_hemispheres(box(0, -10, 10, 0)) == 'SE'\n",
    "test_find_hemispheres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break things up\n",
    "\n",
    "Here we will go thru all the rows in a parquet file. For each file we will use the bounding box \n",
    "(bbox) for the row and calculate the GeoHash for the two corners of that bbox. We will then use those two GeoHash values, which should be 1 character long, to create a 'hash code' for use as the bucket name to store parquet files in with the rows corresponding to that bucket. An example would \n",
    "be `4-g`.\n",
    "\n",
    "Using the lowest precision of 1 will give us 32 grids. From these 32 grids we could have 1024 boxes for every combination of of two bounding box GeoHash codes.\n",
    "\n",
    "Create a GeoDataFrame for every bucket and concat the record to it.\n",
    "\n",
    "Each bucket will also have a file called info.json that will contain the details of the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def flush_parquet(parquet_data:dict, hash_path:str):\n",
    "    \"\"\" write out parquet files and clear out the value in parquet_data. \"\"\"\n",
    "    parquet_file_name = f\"{os.getcwd()}/data2/{hash_path}/data0001.parquet\"\n",
    "\n",
    "    print(f\"{hash_path}->{parquet_file_name}\")\n",
    "    if os.path.exists(parquet_file_name):\n",
    "        existing_data = gpd.read_parquet(parquet_file_name)\n",
    "        clean_data = existing_data.drop(columns=['0', 0], errors='ignore')\n",
    "        combined_data = pd.concat([clean_data, parquet_data[hash_path]], ignore_index=True)\n",
    "        combined_data.to_parquet(parquet_file_name, index=False)\n",
    "    else:\n",
    "        # If file doesn't exist, write new data\n",
    "        parquet_data[hash_path].drop(columns=['0', 0], inplace=True, errors='ignore')\n",
    "        parquet_data[hash_path].to_parquet(parquet_file_name, index=False)\n",
    "    # Update the parquet_data dictionary with an empty GeoDataFrame\n",
    "    parquet_data[hash_path] = gpd.GeoDataFrame(columns=parquet_data[hash_path].columns)\n",
    "\n",
    "def concat_record_to_parquet(parquet_data:dict, hash_path:str, row:pd.Series, columns:pd.Index):\n",
    "  \"\"\" Collect records in a dataframe and flush them out when to many have been stored. \"\"\"\n",
    "  if hash_path not in parquet_data:\n",
    "      parquet_data[hash_path] = gpd.GeoDataFrame(columns=columns)\n",
    "  new_row = pd.DataFrame([row], columns=columns)\n",
    "  #print(new_row.iloc[0].geometry.bounds)\n",
    "  parquet_data[hash_path] = pd.concat([parquet_data[hash_path], new_row], ignore_index=True)\n",
    "  \n",
    "  if len(parquet_data[hash_path]) > 10000:\n",
    "    flush_parquet(parquet_data, hash_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the buckets\n",
    "Do the work to break up the larger parquet file by location. The functions above are helper functions to try to keep the code here small and managable.\n",
    "\n",
    "Last run this took over 5 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "parquet_data = {}\n",
    "\n",
    "partition_start = int(time.time())\n",
    "for index, row in gdf.iterrows():\n",
    "    #if index == 0:\n",
    "    #  print(type(row))\n",
    "    #  print(type(gdf.columns))\n",
    "    #if index < 1000:\n",
    "    #  # skip ahead to some interesting records\n",
    "    #  continue\n",
    "    geometry = row['geometry']\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "    hash1 = pgh.encode(latitude=minx, longitude=miny, precision=3)\n",
    "    hash2 = pgh.encode(latitude=maxx, longitude=maxy, precision=3)\n",
    "    distance = pgh.geohash_approximate_distance(geohash_1=hash1, geohash_2=hash2)\n",
    "    bound_hash = f\"{hash1}-{hash2}\"\n",
    "    hash_path = hash_to_path(hash1, hash2)\n",
    "    details = {'bounds': geometry.bounds,\n",
    "      'hash1': hash1,\n",
    "      'hash2': hash2,\n",
    "      'hash': bound_hash,\n",
    "      'distance': distance}\n",
    "    make_geo_box(hash_path, details)\n",
    "    counter[hash_path] += 1\n",
    "\n",
    "    concat_record_to_parquet(parquet_data, hash_path, row, gdf.columns)\n",
    "    \n",
    "    limit_records = 10000 # 100,000 rows\n",
    "    if len(counter) > limit_records:\n",
    "      print(f\"breaking after {limit_records}\")\n",
    "      break\n",
    "    elif len(counter) % 1000 == 0:\n",
    "      print(f\"{len(counter)} records processed\")\n",
    "print(counter)\n",
    "partition_stop = int(time.time())\n",
    "partition_durration_min = int(partition_stop - partition_start) / 60\n",
    "print (f\"it took {partition_durration}m to partition.\")\n",
    "\n",
    "print('Do not forget to call the final flush below!!!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last\n",
    "Do not forget to call the final flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flush now\")\n",
    "for key, value in parquet_data.items():\n",
    "  flush_parquet(parquet_data, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out buckets\n",
    "\n",
    "Now that we have a dictinary of data frames, write them all out.\n",
    "\n",
    "This has been fast, around 5 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "for key, value in parquet_data.items():\n",
    "    print(f\"writing {key} to disk. {c} of {len(parquet_data.keys())}\")\n",
    "    parquet_data[key].to_parquet(f\"{os.getcwd()}/data/{key}/{key}.parquet\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Run the following code to remove things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just remove the parquet files\n",
    "#!find data2 -name '*.parquet' -exec rm {} \\;\n",
    "\n",
    "# remove the data directory\n",
    "#!rm -rf data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Investigation\n",
    "Now that there is data, lets have a look at it by first picking a box and then looking at the data in that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import duckdb\n",
    "\n",
    "def find_files_with_extension(path, extension):\n",
    "    return [\n",
    "        os.path.join(root, file)\n",
    "        for root, dirs, files in os.walk(path)\n",
    "        for file in files\n",
    "        if file.endswith(extension)\n",
    "    ]\n",
    "\n",
    "def geohashes(boundy_box:box) -> (str, str):\n",
    "  minx, miny, maxx, maxy = boundy_box.bounds\n",
    "  p1 = pgh.encode(latitude=miny, longitude=minx, precision=3)\n",
    "  p2 = pgh.encode(latitude=maxy, longitude=maxx, precision=3)\n",
    "  return p1, p2\n",
    "\n",
    "# Create a shapely box geometry\n",
    "usa = box(-124.848974, 24.396308, -66.885444, 49.384358) # north\n",
    "colorado = box(-109.060251, 36.992424, -102.040899, 41.003444) # one deep\n",
    "rhode_island = box(-71.907258, 41.146339, -71.088571, 42.018798) # 2 deep\n",
    "maryland = box(-79.4871, 37.9130, -75.0491, 39.7229) # one deep\n",
    "baltimore_county = box(-76.711607, 39.214233, -76.347517, 39.721644)\n",
    "baltimore = box(-76.711607, 39.197233, -76.529453, 39.372069) # 3 deep\n",
    "virgina_part = box(-78, 37, -77, 39) # 2 deep\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "usa_bbox = gpd.GeoDataFrame({'name': ['Continental USA']}, \n",
    "                            geometry=[usa], \n",
    "                            crs=\"EPSG:4326\")\n",
    "# Print the GeoDataFrame\n",
    "#print(usa_bbox)\n",
    "\n",
    "p = lambda c, b: print(f\"{c:5}: {geohashes(b)}-> {hash_to_path(*geohashes(b))}\")\n",
    "\n",
    "p(\"USA\", usa)\n",
    "p(\"MD\", maryland)\n",
    "p(\"Virg\", virgina_part)\n",
    "p(\"Balt\", baltimore)\n",
    "\n",
    "sub = hash_to_path(*geohashes(maryland))\n",
    "search_path = f\"{os.getcwd()}/data2/{sub}\"\n",
    "file_list = find_files_with_extension(search_path, \".parquet\")\n",
    "print(f\"maryland file count: {len(file_list)}\")\n",
    "file_list_str = \", \".join(file_list)\n",
    "\n",
    "sql = f'''SELECT COUNT(*) FROM parquet_scan({file_list})'''\n",
    "print(sql)\n",
    "duckdb.sql(sql)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
