{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Brigade - Nested edition\n",
    "\n",
    "Bucket Brigade is a system for taking a large parquet file breaking it up spatially across multiple files using geohash to organize files into nested directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "#import dask_geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from shapely import wkb, wkt\n",
    "import pygeohash as pgh\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a GEOHash\n",
    "\n",
    "Lets take a second to look at geo hash and what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pgh.encode(latitude=42.6, longitude=-5.6))\n",
    "print(pgh.encode(latitude=42.6, longitude=-5.6, precision=5))\n",
    "print(pgh.encode(latitude=32.0, longitude=-7.0, precision=5))\n",
    "\n",
    "print(pgh.decode(geohash='ezs42'))\n",
    "print(pgh.geohash_approximate_distance(geohash_1='bcd3u', geohash_2='bc83n') /1000 ,\"km\")\n",
    "#not found in lib???\n",
    "#print(pgh.get_adjacent(geohash='kd3ybyu', direction='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Pick a file and then read it in using GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file = \"~/src/project/cmr-bigstac-prototype/bigstac/scripts_explore/3mil_no_global_bounds.parquet\"\n",
    "\n",
    "# Read the GeoParquet file\n",
    "gdf = gpd.read_parquet(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions\n",
    "\n",
    "going to use these things latter on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilities\n",
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        #print(f\"Successfully wrote to the file {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file {filename}: {e}\")\n",
    "\n",
    "def make_geo_box(sub:str, details:dict):\n",
    "  \"\"\"Make a directory to store parquet files in\"\"\"\n",
    "  data = f\"{os.getcwd()}/data2/{sub}\"\n",
    "  #print(data)\n",
    "  if not os.path.exists(data):\n",
    "    os.makedirs(data)\n",
    "    write_string_to_file(f\"{data}/info.json\", json.dumps(details))\n",
    "\n",
    "def find_hemispheres(bounds):\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    hemispheres = set()\n",
    "\n",
    "    if minx < 0:\n",
    "        if miny < 0:\n",
    "            hemispheres.add(\"SW\")\n",
    "        if maxy > 0:\n",
    "            hemispheres.add(\"NW\")\n",
    "    if maxx > 0:\n",
    "        if miny < 0:\n",
    "            hemispheres.add(\"SE\")\n",
    "        if maxy > 0:\n",
    "            hemispheres.add(\"NE\")\n",
    "\n",
    "    if len(hemispheres) == 0:\n",
    "        # The box is exactly on the equator and prime meridian\n",
    "        return \"Central\"\n",
    "    elif len(hemispheres) == 4:\n",
    "        return \"All\"\n",
    "    else:\n",
    "        return '-'.join(hemispheres)\n",
    "\n",
    "def is_global(geometry):\n",
    "  minx, miny, maxx, maxy = geometry.bounds\n",
    "  if minx < -180 or maxx > 180 or miny < -90 or maxy > 90:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def hash_to_path(hash1, hash2):\n",
    "  hash_path = ''\n",
    "  # test if box fits in a bin\n",
    "  if hash1 == hash2:\n",
    "    # exact match, three down\n",
    "    hash_path = f\"{hash1[0]}/{hash1[1]}/{hash1[2]}\"\n",
    "  elif hash1[0] == hash2[0] and hash1[1] == hash2[1]:\n",
    "    # match 2 layers down\n",
    "    hash_path = f\"{hash1[0]}/{hash1[1]}\"\n",
    "  elif hash1[0] == hash2[0]:\n",
    "    # match 1 layer down\n",
    "    hash_path = f\"{hash1[0]}/{hash1[1]}\"\n",
    "  elif is_global(geometry):\n",
    "    # global\n",
    "      hash_path = \"global\"\n",
    "  else:\n",
    "    # does not fit in just one box, check hemispheres\n",
    "    hash_path = find_hemispheres(geometry.bounds)\n",
    "  \n",
    "  return hash_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your really worried about the code above, run this otherwise minimize it and forget it\n",
    "from shapely.geometry import Polygon\n",
    "def test_find_hemispheres():\n",
    "  assert find_hemispheres((-10, -10, 1, 1)) == 'All'\n",
    "  assert find_hemispheres((-10, -10, -5, -5)) == 'SW'\n",
    "  assert find_hemispheres((5, -10, 10, -5)) == 'SE'\n",
    "  assert find_hemispheres((-10, 5, -5, 10)) == 'NW'\n",
    "  assert find_hemispheres((5, 5, 10, 10)) == 'NE'\n",
    "  assert find_hemispheres((-10, -10, 10, 0)) == 'SW-SE'\n",
    "  assert find_hemispheres((-10, 0, 10, 10)) == 'NW-NE'\n",
    "  assert find_hemispheres((-10, -10, 0, 10)) == 'NW-SW'\n",
    "  assert find_hemispheres((0, -10, 10, 10)) == 'NE-SE'\n",
    "  assert find_hemispheres((-10, 0, 0, 10)) == 'NW'\n",
    "  assert find_hemispheres((0, -10, 10, 0)) == 'SE'\n",
    "  assert find_hemispheres((-10, -10, 0, 0)) == 'SW'\n",
    "  assert find_hemispheres((0, 0, 10, 10)) == 'NE'\n",
    "  assert find_hemispheres((-10, 0, 0, 10)) == 'NW'\n",
    "  assert find_hemispheres((0, -10, 10, 0)) == 'SE'\n",
    "  assert find_hemispheres((-10, -10, 0, 0)) == 'SW'\n",
    "  assert find_hemispheres((0, 0, 10, 10)) == 'NE'\n",
    "  assert find_hemispheres((-10, 0, 0, 10)) == 'NW'\n",
    "  assert find_hemispheres((0, -10, 10, 0)) == 'SE'\n",
    "test_find_hemispheres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break things up\n",
    "\n",
    "Here we will go thru all the rows in a parquet file. For each file we will use the bounding box \n",
    "(bbox) for the row and calculate the GeoHash for the two corners of that bbox. We will then use those two GeoHash values, which should be 1 character long, to create a 'hash code' for use as the bucket name to store parquet files in with the rows corresponding to that bucket. An example would \n",
    "be `4-g`.\n",
    "\n",
    "Using the lowest precision of 1 will give us 32 grids. From these 32 grids we could have 1024 boxes for every combination of of two bounding box GeoHash codes.\n",
    "\n",
    "Create a GeoDataFrame for every bucket and concat the record to it.\n",
    "\n",
    "Each bucket will also have a file called info.json that will contain the details of the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "\n",
    "def flush_parquet(parquet_data:dict, hash_path:str):\n",
    "    \"\"\" write out parquet files and clear out the value in parquet_data. \"\"\"\n",
    "    parquet_file_name = f\"{os.getcwd()}/data2/{hash_path}/data0001.parquet\"\n",
    "\n",
    "    print(f\"{hash_path}->{parquet_file_name}\")\n",
    "    if os.path.exists(parquet_file_name):\n",
    "        existing_data = gpd.read_parquet(parquet_file_name)\n",
    "        clean_data = existing_data.drop(columns=['0', 0], errors='ignore')\n",
    "        combined_data = pd.concat([clean_data, parquet_data[hash_path]], ignore_index=True)\n",
    "        combined_data.to_parquet(parquet_file_name, index=False)\n",
    "    else:\n",
    "        # If file doesn't exist, write new data\n",
    "        parquet_data[hash_path].drop(columns=['0', 0], inplace=True, errors='ignore')\n",
    "        parquet_data[hash_path].to_parquet(parquet_file_name, index=False)\n",
    "    # Update the parquet_data dictionary with an empty GeoDataFrame\n",
    "    parquet_data[hash_path] = gpd.GeoDataFrame(columns=parquet_data[hash_path].columns)\n",
    "\n",
    "def concat_record_to_parquet(parquet_data:dict, hash_path:str, row:pd.Series, columns:pd.Index):\n",
    "  \"\"\" Collect records in a dataframe and flush them out when to many have been stored. \"\"\"\n",
    "  if hash_path not in parquet_data:\n",
    "      parquet_data[hash_path] = gpd.GeoDataFrame(columns=columns)\n",
    "  new_row = pd.DataFrame([row], columns=columns)\n",
    "  #print(new_row.iloc[0].geometry.bounds)\n",
    "  parquet_data[hash_path] = pd.concat([parquet_data[hash_path], new_row], ignore_index=True)\n",
    "  \n",
    "  if len(parquet_data[hash_path]) > 10000:\n",
    "    flush_parquet(parquet_data, hash_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small investigations, delete these\n",
    "\n",
    "for key, value in parquet_data.items():\n",
    "  print(value['geometry'].bounds)\n",
    "  break\n",
    "\n",
    "print(\"*\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "parquet_data = {}\n",
    "\n",
    "for index, row in gdf.iterrows():\n",
    "    if index == 0:\n",
    "      print(type(row))\n",
    "      print(type(gdf.columns))\n",
    "    if index < 1000:\n",
    "      # skip ahead to some interesting records\n",
    "      continue\n",
    "    geometry = row['geometry']\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "    hash1 = pgh.encode(latitude=minx, longitude=miny, precision=3)\n",
    "    hash2 = pgh.encode(latitude=maxx, longitude=maxy, precision=3)\n",
    "    distance = pgh.geohash_approximate_distance(geohash_1=hash1, geohash_2=hash2)\n",
    "    bound_hash = f\"{hash1}-{hash2}\"\n",
    "    hash_path = hash_to_path(hash1, hash2)\n",
    "    details = {'bounds': geometry.bounds,\n",
    "      'hash1': hash1,\n",
    "      'hash2': hash2,\n",
    "      'hash': bound_hash,\n",
    "      'distance': distance}\n",
    "    make_geo_box(hash_path, details)\n",
    "    counter[hash_path] += 1\n",
    "\n",
    "    concat_record_to_parquet(parquet_data, hash_path, row, gdf.columns)\n",
    "    \n",
    "    limit_records = 10000 # 100,000 rows\n",
    "    if len(counter) > limit_records:\n",
    "      print(f\"breaking after {limit_records}\")\n",
    "      break\n",
    "    elif len(counter) % 1000 == 0:\n",
    "      print(f\"{len(counter)} records processed\")\n",
    "print(counter)\n",
    "\n",
    "print('Do not forget to call the final flush below!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flush now\")\n",
    "for key, value in parquet_data.items():\n",
    "  flush_parquet(parquet_data, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out buckets\n",
    "\n",
    "Now that we have a dictinary of data frames, write them all out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "for key, value in parquet_data.items():\n",
    "    print(f\"writing {key} to disk. {c} of {len(parquet_data.keys())}\")\n",
    "    parquet_data[key].to_parquet(f\"{os.getcwd()}/data/{key}/{key}.parquet\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Run the following code to remove things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just remove the parquet files\n",
    "#!find data2 -name '*.parquet' -exec rm {} \\;\n",
    "\n",
    "# remove the data directory\n",
    "#!rm -rf data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at one of these boxes, just so we know what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = geometry\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the polygon\n",
    "x, y = polygon.exterior.xy\n",
    "ax.plot(x, y)\n",
    "\n",
    "# Fill the polygon\n",
    "ax.fill(x, y, alpha=0.3)\n",
    "\n",
    "# Set the aspect of the plot to equal\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Add title\n",
    "ax.set_title(\"Polygon Visualization\")\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
