{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geohash binning on large dataset\n",
    "\n",
    "Uses a Python user-defined function in DuckDB to calculate spatial bins by global, hemisphere, and 1-character geohashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from duckdb.typing import *\n",
    "import geohash_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file = \"../../dev/test_lpcloud_data/single_file/merged_sorted_ddb_75mil.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare 75m granule data\n",
    "\n",
    "There are duplicate GranuleURs in this dataset due to how complex geometries were handled by the harvesting script. These duplicates would create issues later when joining bins, so we'll create a new version with duplicates removed.  \n",
    "\n",
    "First, create a local DB to store the calculation result so we don't lose it if we crash later.  \n",
    "Then, register the geohash binning function to the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/gh88d4px3f93zz2jf81_cl0m0000gp/T/ipykernel_58509/3221329991.py:2: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  db_con.create_function('hashbin_wkb', geohash_bin.wkb_to_hash_path, [BLOB, BIGINT], VARCHAR)\n"
     ]
    }
   ],
   "source": [
    "db_con = duckdb.connect('geohash.db')\n",
    "db_con.create_function('hashbin_wkb', geohash_bin.wkb_to_hash_path, [BLOB, BIGINT], VARCHAR)\n",
    "%load_ext sql\n",
    "%sql db_con --alias duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, create a view that references the input 75m granule parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Count</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------+\n",
       "| Count |\n",
       "+-------+\n",
       "+-------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "LOAD spatial;\n",
    "CREATE OR REPLACE VIEW pq75m\n",
    "AS FROM (\n",
    "  SELECT * FROM read_parquet('{{selected_file}}')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granule count in the harvested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>count_star()</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>75000314</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+--------------+\n",
       "| count_star() |\n",
       "+--------------+\n",
       "|   75000314   |\n",
       "+--------------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT COUNT(*) FROM pq75m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex shapes (e.g. multipolygons, or polygons with holes) were split into multiple records, one for each constituent shape. We'll subset to a single version of each GranuleUR, keeping the largest shape by area for each granule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Count</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>17648544</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+\n",
       "|  Count   |\n",
       "+----------+\n",
       "| 17648544 |\n",
       "+----------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE unduped AS FROM (\n",
    "  SELECT DISTINCT ON(GranuleUR) *\n",
    "  FROM pq75m\n",
    "  ORDER BY ST_Area(geometry)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply geohash binning to deduplicated granules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unduplicated table stores the geometry column as DuckDB Spatial's `GEOMETRY` type. That isn't supported by Python user-defined functions (UDF), so we'll convert it back to WKB to send to the binning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Count</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>17648544</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+\n",
       "|  Count   |\n",
       "+----------+\n",
       "| 17648544 |\n",
       "+----------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE binned\n",
    "AS FROM (\n",
    "  SELECT GranuleUR, hashbin_wkb(ST_AsWKB(geometry), 1) AS hashbin FROM unduped\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the binning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>GranuleUR</th>\n",
       "            <th>hashbin</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>92916a38-b8dd-42ba-b4bf-05587a6f1236</td>\n",
       "            <td>9</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0382980e-726e-433a-8031-ac491a8d2850</td>\n",
       "            <td>NE-SE</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>aa9a84c5-e22b-4a82-b240-e5ac3bbbf943</td>\n",
       "            <td>All</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>sc:g3bt.052:64110734</td>\n",
       "            <td>2</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6ae0b974-13d3-4d98-8a90-248665dff540</td>\n",
       "            <td>All</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+--------------------------------------+---------+\n",
       "|              GranuleUR               | hashbin |\n",
       "+--------------------------------------+---------+\n",
       "| 92916a38-b8dd-42ba-b4bf-05587a6f1236 |    9    |\n",
       "| 0382980e-726e-433a-8031-ac491a8d2850 |  NE-SE  |\n",
       "| aa9a84c5-e22b-4a82-b240-e5ac3bbbf943 |   All   |\n",
       "|         sc:g3bt.052:64110734         |    2    |\n",
       "| 6ae0b974-13d3-4d98-8a90-248665dff540 |   All   |\n",
       "+--------------------------------------+---------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM binned LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of bin counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bins_df = db_con.sql(\"SELECT * FROM binned\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = sorted(bins_df['hashbin'].unique())\n",
    "i2 = ['All', 'NE-NW', 'NE', 'NW', 'NE-SE', 'SW-NW', 'SW', 'SE', 'SW-SE']\n",
    "i3 = i2 + [x for x in i1 if x not in i2]\n",
    "cat_type = pd.CategoricalDtype(categories=i3, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashbin\n",
       "All      9320146\n",
       "NE-NW     724015\n",
       "NE        672565\n",
       "NW        734480\n",
       "NE-SE     850102\n",
       "SW-NW     702775\n",
       "SW        418319\n",
       "SE        374589\n",
       "SW-SE     581949\n",
       "0          62657\n",
       "1          56889\n",
       "2         118231\n",
       "3          56586\n",
       "4          66850\n",
       "5          66470\n",
       "6         129823\n",
       "7          79986\n",
       "8          79953\n",
       "9         289086\n",
       "b         141301\n",
       "c          89618\n",
       "d         252633\n",
       "e         101341\n",
       "f          76693\n",
       "g          82358\n",
       "h          35535\n",
       "j          43738\n",
       "k         119659\n",
       "m          78774\n",
       "n          38977\n",
       "p          73045\n",
       "q         108962\n",
       "r         132801\n",
       "s         143829\n",
       "t         166878\n",
       "u          78043\n",
       "v          74501\n",
       "w         195655\n",
       "x         120315\n",
       "y          42042\n",
       "z          66375\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_counts = bins_df['hashbin'].astype(cat_type).value_counts().sort_index()\n",
    "bin_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the geohash binning table and export each bin sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'geohash_bins'\n",
    "\n",
    "import os\n",
    "os.makedirs(os.path.join(outdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb4654118be46e68d8b1f008dce2f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for export_bin in i3:\n",
    "  db_con.sql(f\"\"\"\n",
    "          COPY (\n",
    "            SELECT unduped.*, binned.hashbin\n",
    "            FROM unduped\n",
    "            JOIN binned\n",
    "            ON unduped.GranuleUR = binned.GranuleUR\n",
    "            WHERE hashbin = '{export_bin}'\n",
    "          )\n",
    "          TO '{os.path.join(outdir, export_bin + \".parquet\")}'\n",
    "          (FORMAT 'parquet', ROW_GROUP_SIZE 100_000)\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the output parquet files\n",
    "\n",
    "Note the row group size parameter set to 100,000 above seems to be ignored when it would result in fewer than 10 row groups per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition  rows     row_groups  size_MB\n",
      "All        9320146  99          1175\n",
      "NE-NW      724015   10          108\n",
      "NE-SE      850102   15          130\n",
      "NE         672565   11          92\n",
      "NW         734480   12          127\n",
      "SE         374589   10          53\n",
      "SW-NW      702775   11          110\n",
      "SW-SE      581949   10          88\n",
      "SW         418319   10          63\n",
      "0          62657    10          10\n",
      "1          56889    10          8\n",
      "2          118231   10          15\n",
      "3          56586    10          7\n",
      "4          66850    10          10\n",
      "5          66470    10          9\n",
      "6          129823   10          15\n",
      "7          79986    10          9\n",
      "8          79953    10          10\n",
      "9          289086   10          87\n",
      "b          141301   10          21\n",
      "c          89618    10          13\n",
      "d          252633   10          39\n",
      "e          101341   10          13\n",
      "f          76693    10          17\n",
      "g          82358    10          16\n",
      "h          35535    10          5\n",
      "j          43738    10          6\n",
      "k          119659   10          14\n",
      "m          78774    10          10\n",
      "n          38977    10          6\n",
      "p          73045    10          12\n",
      "q          108962   10          13\n",
      "r          132801   10          17\n",
      "s          143829   10          17\n",
      "t          166878   10          19\n",
      "u          78043    10          15\n",
      "v          74501    10          13\n",
      "w          195655   10          31\n",
      "x          120315   10          15\n",
      "y          42042    10          7\n",
      "z          66375    10          12\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd geohash_bins/\n",
    "echo -e \"$(ls [A-Z]*.parquet)\" \"\\n\" \"$(ls [0-9]*.parquet)\" \"\\n\" \"$(ls [a-z]*.parquet)\" > filenames.txt\n",
    "echo \"partition,rows,row_groups,size_MB\" > parquet_stats.csv\n",
    "while read f; do\n",
    "  echo -n \"$(basename $f .parquet)\"\",\"; ~/apps/parquet-tools row-count $f| \\\n",
    "  tr '\\n' ','; ~/apps/parquet-tools meta $f | jq '.NumRowGroups'| \\\n",
    "  tr '\\n' ','; du -m $f | awk '{print $1}'; \\\n",
    "  done < filenames.txt >> parquet_stats.csv\n",
    "rm filenames.txt\n",
    "column -s',' -t parquet_stats.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigstac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
