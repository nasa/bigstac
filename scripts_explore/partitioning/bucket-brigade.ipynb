{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Brigade\n",
    "\n",
    "Bucket Brigade is a system for taking a large parquet file breaking it up spatially across multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "import numpy\n",
    "import numba\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "#import dask_geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from shapely import wkb, wkt\n",
    "import pygeohash as pgh\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = widgets.IntSlider(min=-10, max=30, step=1, value=10)\n",
    "display(thing)\n",
    "\n",
    "\n",
    "AttributeError: The geopandas.dataset has been deprecated and was removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a GEOHash\n",
    "\n",
    "Lets take a second to look at geo hash and what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pgh.encode(latitude=42.6, longitude=-5.6))\n",
    "print(pgh.encode(latitude=42.6, longitude=-5.6, precision=5))\n",
    "print(pgh.decode(geohash='ezs42'))\n",
    "print(pgh.geohash_approximate_distance(geohash_1='bcd3u', geohash_2='bc83n') /1000 ,\"km\")\n",
    "\n",
    "print(pgh.encode(latitude=42.0, longitude=-6))\n",
    "print(pgh.encode(latitude=42.6, longitude=-5.6))\n",
    "print(\"*\"*5)\n",
    "\n",
    "def show_two(latitude, longitude, msg):\n",
    "  normal = pgh.encode(latitude=latitude, longitude=longitude, precision=5)\n",
    "  strict = pgh.encode_strictly(latitude=latitude, longitude=longitude, precision=5)\n",
    "  print(f\"{normal} vs {strict} : {msg}\")\n",
    "\n",
    "show_two(latitude=0.0, longitude=-180.0, msg=\"eq west\") \n",
    "show_two(latitude=-90.0, longitude=180.0, msg=\"SE\") #not correct\n",
    "show_two(latitude=0.0, longitude=180.0, msg=\"eq east\") # not correct\n",
    "show_two(latitude=90.0, longitude=180.0, msg=\"NE\") # not correct\n",
    "show_two(latitude=-90.0, longitude=0.0, msg=\"south central\")\n",
    "show_two(latitude=0.0, longitude=-5.6, msg=\"just south\")\n",
    "\n",
    "show_two(latitude=-89.99, longitude=179.99, msg=\"SE, close\")\n",
    "show_two(latitude=0.0, longitude=179.99, msg=\"eq east, close\")\n",
    "show_two(latitude=89.99, longitude=179.99, msg=\"NE, close\")\n",
    "\n",
    "#not found in lib???\n",
    "print(pgh.get_adjacent(geohash='kd3ybyu', direction='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Pick a file and then read it in using GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file = \"~/src/project/cmr-bigstac-prototype/bigstac/scripts_explore/3mil_no_global_bounds.parquet\"\n",
    "\n",
    "# Read the GeoParquet file\n",
    "gdf = gpd.read_parquet(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions\n",
    "\n",
    "going to use these things latter on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilities\n",
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        #print(f\"Successfully wrote to the file {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file {filename}: {e}\")\n",
    "\n",
    "def make_geo_box(sub:str, details:dict):\n",
    "  data = f\"{os.getcwd()}/data/{sub}\"\n",
    "  if not os.path.exists(data):\n",
    "    os.makedirs(data)\n",
    "    write_string_to_file(f\"{data}/info.json\", json.dumps(details))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break things up\n",
    "\n",
    "Here we will go thru all the rows in a parquet file. For each file we will use the bounding box \n",
    "(bbox) for the row and calculate the GeoHash for the two corners of that bbox. We will then use those two GeoHash values, which should be 1 character long, to create a 'hash code' for use as the bucket name to store parquet files in with the rows corresponding to that bucket. An example would \n",
    "be `4-g`.\n",
    "\n",
    "Using the lowest precision of 1 will give us 32 grids. From these 32 grids we could have 992 boxes (nPr where n=32, r=2) boxes for every combination of of two bounding box GeoHash codes.\n",
    "\n",
    "Create a GeoDataFrame for every bucket and concat the record to it.\n",
    "\n",
    "Each bucket will also have a file called info.json that will contain the details of the bucket.\n",
    "\n",
    "Numbers that are used (no a, i, l, o):\n",
    "\n",
    "| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7  \n",
    "|---|---|---|---|---|---|---|---\n",
    "| 8 | 9 | b | c | d | e | f | g  \n",
    "| h | j | k | m | n | p | q | r  \n",
    "| s | t | u | v | w | x | y | z  \n",
    "\n",
    "Layout on the Earth is like this:\n",
    "\n",
    "North\n",
    "\n",
    "| b | c | f | g | u | v | y | z  \n",
    "|---|---|---|---|---|---|---|---\n",
    "| 8 | 9 | d | e | s | e | t | x  \n",
    "| 2 | 3 | 6 | 7 | k | p | m | r  \n",
    "| 0 | 1 | 4 | 5 | h | j | n | p  \n",
    "\n",
    "South"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over all rows\n",
    "Iterate over all the rows and sort them into their buckets based on their geohash codes for the\n",
    "bounding box of that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "parquet_data = {}\n",
    "\n",
    "for index, row in gdf.iterrows():\n",
    "    geometry = row['geometry']\n",
    "    \n",
    "    # Access other attributes\n",
    "    for column in gdf.columns:\n",
    "        if column != 'geometry':\n",
    "            value = row[column]\n",
    "            #print(f\"{column}: {value}\")\n",
    "            break\n",
    "\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "\n",
    "    hash1 = pgh.encode(latitude=minx, longitude=miny, precision=1)\n",
    "    hash2 = pgh.encode(latitude=maxx, longitude=maxy, precision=1)\n",
    "    distance = pgh.geohash_approximate_distance(geohash_1=parts[0], geohash_2=parts[1])\n",
    "    hash = f\"{hash1}-{hash2}\"\n",
    "    details = {'hash1': hash1,\n",
    "      'hash2': hash2,\n",
    "      'hash': hash,\n",
    "      'distance': distance,\n",
    "      'bounds': geometry.bounds}\n",
    "    make_geo_box(hash, details)\n",
    "    counter[hash] += 1\n",
    "\n",
    "    # maybe change this to instead open and write the file in one function so that nothing is in\n",
    "    # memory.\n",
    "    if hash not in parquet_data:\n",
    "      if os.path.exists(f\"{os.getcwd()}/data/{hash}/{hash}.parquet\"):\n",
    "        print(f\"reading {hash} from disk\")\n",
    "        parquet_data[hash] = gpd.read_parquet(selected_file)\n",
    "      else:\n",
    "        print(f\"creating a new dataframe as {hash}\")\n",
    "        parquet_data[hash] = gpd.GeoDataFrame()\n",
    "    #add this record\n",
    "    parquet_data[hash] = gpd.GeoDataFrame(pd.concat([parquet_data[hash], pd.DataFrame([row])], ignore_index=True))\n",
    "    \n",
    "    limit_records = 100000 # 100,000 rows\n",
    "    if len(counter) > limit_records:\n",
    "      print(f\"breaking after {limit_records}\")\n",
    "      break\n",
    "    elif len(counter) % 10000 == 0:\n",
    "      print(f\"{len(counter)} records processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out buckets\n",
    "\n",
    "Now that we have a dictinary of data frames, write them all out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "for key, value in parquet_data.items():\n",
    "    print(f\"writing {key} to disk. {c} of {len(parquet_data.keys())}\")\n",
    "    parquet_data[key].to_parquet(f\"{os.getcwd()}/data/{key}/{key}.parquet\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at one of these boxes, just so we know what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = geometry\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the polygon\n",
    "x, y = polygon.exterior.xy\n",
    "ax.plot(x, y)\n",
    "\n",
    "# Fill the polygon\n",
    "ax.fill(x, y, alpha=0.3)\n",
    "\n",
    "# Set the aspect of the plot to equal\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Add title\n",
    "ax.set_title(\"Polygon Visualization\")\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
