{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Brigade\n",
    "\n",
    "Bucket Brigade is a system for taking a large parquet file breaking it up spatially across multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "#import dask_geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from shapely import wkb, wkt\n",
    "import pygeohash as pgh\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a GEOHash\n",
    "\n",
    "Lets take a second to look at geo hash and what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pgh.encode(latitude=42.6, longitude=-5.6))\n",
    "print(pgh.encode(latitude=42.6, longitude=-5.6, precision=5))\n",
    "print(pgh.decode(geohash='ezs42'))\n",
    "print(pgh.geohash_approximate_distance(geohash_1='bcd3u', geohash_2='bc83n') /1000 ,\"km\")\n",
    "#not found in lib???\n",
    "#print(pgh.get_adjacent(geohash='kd3ybyu', direction='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Pick a file and then read it in using GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file = \"~/src/project/cmr-bigstac-prototype/bigstac/scripts_explore/3mil_no_global_bounds.parquet\"\n",
    "\n",
    "# Read the GeoParquet file\n",
    "gdf = gpd.read_parquet(selected_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions\n",
    "\n",
    "going to use these things latter on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilities\n",
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        #print(f\"Successfully wrote to the file {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file {filename}: {e}\")\n",
    "\n",
    "def make_geo_box(sub:str, details:dict):\n",
    "  data = f\"{os.getcwd()}/data/{sub}\"\n",
    "  if not os.path.exists(data):\n",
    "    os.makedirs(data)\n",
    "    write_string_to_file(f\"{data}/info.json\", json.dumps(details))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break things up\n",
    "\n",
    "Here we will go thru all the rows in a parquet file. For each file we will use the bounding box for\n",
    "the row and calculate the GeoHash for the two corners. We will then use the GeoHash to create a\n",
    "'hash code' for the bucket to store rows in.\n",
    "\n",
    "Using the lowest precision of 1 will give us 32 grids. From these 32 grids we could have 1024 boxes for every combination of of two bounding box GeoHash codes.\n",
    "\n",
    "Create a GeoDataFrame for every bucket and concat the record to it.\n",
    "\n",
    "Each bucket will also have a file called info.json that will contain the details of the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "parquet_data = {}\n",
    "\n",
    "for index, row in gdf.iterrows():\n",
    "    geometry = row['geometry']\n",
    "    \n",
    "    # Access other attributes\n",
    "    for column in gdf.columns:\n",
    "        if column != 'geometry':\n",
    "            value = row[column]\n",
    "            #print(f\"{column}: {value}\")\n",
    "            break\n",
    "\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "\n",
    "    hash1 = pgh.encode(latitude=minx, longitude=miny, precision=1)\n",
    "    hash2 = pgh.encode(latitude=maxx, longitude=maxy, precision=1)\n",
    "    distance = pgh.geohash_approximate_distance(geohash_1=parts[0], geohash_2=parts[1])\n",
    "    hash = f\"{hash1}-{hash2}\"\n",
    "    details = {'hash1': hash1,\n",
    "      'hash2': hash2,\n",
    "      'hash': hash,\n",
    "      'distance': distance,\n",
    "      'bounds': geometry.bounds}\n",
    "    make_geo_box(hash, details)\n",
    "    counter[hash] += 1\n",
    "\n",
    "    # maybe change this to instead open and write the file in one function so that nothing is in\n",
    "    # memory.\n",
    "    if hash not in parquet_data:\n",
    "      if os.path.exists(f\"{os.getcwd()}/data/{hash}/{hash}.parquet\"):\n",
    "        print(f\"reading {hash} from disk\")\n",
    "        parquet_data[hash] = gpd.read_parquet(selected_file)\n",
    "      else:\n",
    "        print(f\"creating a new dataframe as {hash}\")\n",
    "        parquet_data[hash] = gpd.GeoDataFrame()\n",
    "    #add this record\n",
    "    parquet_data[hash] = gpd.GeoDataFrame(pd.concat([parquet_data[hash], pd.DataFrame([row])], ignore_index=True))\n",
    "    \n",
    "    limit_records = 500000\n",
    "    if len(counter) > limit_records:\n",
    "      print(f\"breaking after {limit_records}\")\n",
    "      break\n",
    "    elif len(counter) % 10000 == 0:\n",
    "      print(f\"{len(counter)} records processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "for key, value in parquet_data.items():\n",
    "    print(f\"writing {key} to disk. {c} of {len(parquet_data.keys())}\")\n",
    "    parquet_data[key].to_parquet(f\"{os.getcwd()}/data/{key}/{key}.parquet\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f\"{os.getcwd()}/data\"\n",
    "\n",
    "if not os.path.exists(data):\n",
    "  os.makedirs(data)\n",
    "\n",
    "for i in counter:\n",
    "  if not os.path.exists(f\"{data}/{i}\"):\n",
    "    os.makedirs(f\"{data}/{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at one of these boxes, just so we know what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = geometry\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the polygon\n",
    "x, y = polygon.exterior.xy\n",
    "ax.plot(x, y)\n",
    "\n",
    "# Fill the polygon\n",
    "ax.fill(x, y, alpha=0.3)\n",
    "\n",
    "# Set the aspect of the plot to equal\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Add title\n",
    "ax.set_title(\"Polygon Visualization\")\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
